* improve coding style:
	- comments "//----" vs "/////"
	- more consistent templated classes: t<xxx> as inputs, t<xxx> as derived

* convolution_layer<int krows, int kcols>
- conv4x4, conv8x8, conv12x12, conv16x16

=> can make the code much faster
=> split the sumation (use two accumulators instead a single sum)

* change the convolution layer:
	- make the ::backward function templated after the convolution size (similar to conv_add)
		- maybe move it to deconvolution.hpp!

		- it could be faster if using the same technique as with conv_add_dynamic
				
* serializer: store data in std::vector, push_back values -> (serializer_t() << s << v << m).data() or .size()
* same for deserializer
	- BETTER name for serializer! -> vectorizer?!
	
idea: variational learning (ebbbost)
idea: regularized (bound) loss with the same approach as in nips12a.pdf
	

* svhn decoding:
	- the sections are not decoded correctly: 
		- the data type of the last two sections is not valid
		- the number of bytes in the last section is not valid


* test program to print model (ncv_describe -> model structure + convolutions as images)

* thread_loop with states should use std::future to obtain the returned values (e.g. gradient & loss value)

* could use std::atexit() to save the model at closing -> re-training should reload the current model and continue training
* better could catch various signals: http://en.cppreference.com/w/cpp/utility/program and safely saved the trained model

* visualize convolutions after/during training either using a Qt ui or some images

- perf result:   
    66.69%  ncv_trainer  libnanocv.so         [.] ncv::conv_layer_t::backward(ncv::tensor3d_t const&) const                                                                  
    26.76%  ncv_trainer  libnanocv.so         [.] ncv::conv_layer_t::forward(ncv::tensor3d_t const&) const                                                                   
     3.14%  ncv_trainer  libnanocv.so         [.] ncv::fun1_activation_t::value(double) const                                                                                
     1.07%  ncv_trainer  libnanocv.so         [.] ncv::fun1_activation_t::vgrad(double) const

* make install:
	- also copy the headers 

- update the experimentation script:
	- evaluate different architecture: vary the number of layers, the activation functions and switch on and off the pooling

- implement other training methods for the convolution network:
	- auto-encoding: using the unlabeled data (could also try with all samples)
	- train a single layer at a time
	- regularization: l2, symmetry
	- could use genetic algorithms and encode a convolution matrix using e.g. 16 possible values, can impose symmetry-like constrains 
	- could add options to filter the input: e.g. smoothing, normalization, contrast equalization

	- autoencoders: how to generate samples ?!

- small ui application to visualize task images

- faster RGB to CIELab transform (goal: 1s single-thread version of the color testing program)

- add other datasets:
	- INRIA car and person datasets
	- NORB, inria pedestrian detection, inria human detection, dymlerchrysler human detection, face detection, PASCALVoc2011, CALTECH101...

