- BUG: NAG is not working well (nans & infs in the test program)

- refactor criteria:
	- L2-norm: update the gradient per sample with L2/N to be usable with different stochastic methods

- other stochastic methods:
	- accelerated gradient (Nesterov): http://blogs.princeton.edu/imabandit/2013/04/01/acceleratedgradientdescent/
	- http://www.cs.berkeley.edu/~jduchi/projects/DuchiHaSi11.pdf
	
	- second-order: apparently generalizes as well as the empirical optimum in a SINGLE pass
	- Leon-Bottou - one pass ...

- stochastic methods using adaptive learning rates for each dimensions:
	- if gradient becomes smaller, than decrease the learning rate ...

- MAD with index shifts (may speed-up corr2d)

- test program to assess the speed of the 3D convolution/correlation operations

- faster convolution:
 	- build a toeplitz matrix, then the convolution becomes a matrix multiplication:
		- http://en.wikipedia.org/wiki/Toeplitz_matrix

        - check if any improvement if using a 3D convolution    

- Tikhonov regularizer	

- finish the training scripts for other datasets (CIFAR-10, CIFAR-100, SVHN, NORB, STL)

- GTSD dataset support (after 0.1 release):
	- create both classification and detection tasks
	- build an object detector

- unsupervised learning (after 0.1 release): 
	- unsupervised training:
		- new loss (without the output layer): a single output should have a much larger value magnitude than the others 
			(e.g. learn to disintangle the variation modes)
		- than train the output layer with the annotated samples

- reconstruction regularization (after 0.1 release):
	- for linear & convolution layers -> reuse parameters to easily reconstruct the input

- feature visualization & image generation (after 0.1 release)

- random forests (after 0.1 release)

- thread_loop with states should use std::future to obtain the returned values (e.g. gradient & loss value)
	

