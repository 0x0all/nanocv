- better names for the activation functions

- test program to check the backward & forward step:
	- double check the gradient backward part of the activation functions
		and the math at the layer::backward() part

	- conclusion: 
		- the gradient estimation error increases with both the input range and the number of layers

	- check for better approximations of the gradient!

	- the tanh saturates very fast (needs smaller inputs)!

	- still problems with maxzero!	

- improve serialization interface: (serializer_t(size) << layers).data():
	- also cleanup the interface (too many functions are overloaded)

	- EASIER method to convert to/from tensors and vectors

- MACRO to simplify the clonable_t interface implementation (e.g. loss, activation, task ...)

- convolution model parameters:
	- number of iterations and epsilon
	- save this to model file (the number of iterations remaining)

- train -> save at closing -> load model -> train from the last parameters & remaining optimization iterations

- MUCH faster optimization (profile the code, it takes more time than without tensors):
	- use noalias for block -> check if any improvement:
		- mat.noalias().block() += ...	

	- bug: the loss becomes infinity sometimes (the line-search step overshoots?!)
	- bug: too many line-search steps => the gradient may be incorrectly computed

	- focus on the backward step: it takes 50% of the processing time
	
	- checkout the convergence speed, experiment with gradient descent and conjugate gd

	- 6s:xxxms for 5000 samples & 8:8x8 hidden layer!
	- 2s:600ms for 2000 samples & 8:8x8 hidden layer!

- option to save the model at closing the program - destructor:
	- trainer().set_path().train()
		- use random initialization if the path is invalid
	- otherwise continue training

- constrain the loss such that the optimization works properly when the number of parameters
	is larger than the number of samples

- convolution network:
	- implement the LBFGS / backprop trainer
	- choose activation from command line

	- make the affine model a specialization of the convolution layer (e.g. no hidden layer)

* image_t::load(matrix_t, color_min = black, color_max = white) -> save the given matrix/filter as a grayscale (by default) image
	- can be used to verify the learned weights/convolutions

- install:
	- also copy the headers 

- update the experimentation script:
	- evaluate some small networks (with 0/1/2... hidden layers), make sure that it works for no hidden layers (like before)
	- evaluate some activation functions

- implement other training methods for the convolution network:
	- >>> derive from the base convolution model and override ::train()
	- auto-encoding: using the unlabeled data (could also try with all samples)
	- train a single layer at a time
	- regularization: l2, symmetry
	- could use genetic algorithms and encode a convolution matrix using e.g. 16 possible values, can impose symmetry-like constrains 
	- could add options to filter the input: e.g. smoothing, normalization, contrast equalization

	- autoencoders: how to generate samples ?!

- small ui application to visualize task images

- faster RGB to CIELab transform (goal: 1s single-thread version of the color testing program)

- add other datasets:
	- INRIA car and person datasets
	- NORB, inria pedestrian detection, inria human detection, dymlerchrysler human detection, face detection, PASCALVoc2011, CALTECH101...

