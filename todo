- new convolution-based layers:
	- (1) - keep the convolution random -> learn their weights
	- (2) - O(o) = sum(i, m(o,i) * I(i) @ C(o, i)), where m(o,i) is a fixed discrete {0, 1} random mask -> learn the convolutions
	- (3) - fixed random convolutions -> learn only the output linear layers

- faster convolution:
        - check if any improvement if using a 3D convolution    

- ctest-based unit tests

- documentation using:
	- https://docs.readthedocs.org/en/latest/getting_started.html
               
- finish the training scripts for other datasets (CIFAR-10, CIFAR-100, SVHN, NORB, STL)

- improve the speed of the linear layers: 
	- the gradient takes too much to compute!
	- the gradient does not scale with the number of threads

- GTSD dataset support (after 0.1 release)

- unsupervised learning (after 0.1 release): 
	- unsupervised training:
		- new loss (without the output layer): a single output should have a much larger value magnitude than the others 
			(e.g. learn to disintangle the variation modes)
		- than train the output layer with the annotated samples

- reconstruction regularization (after 0.1 release):
	- for linear & convolution layers -> reuse parameters to easily reconstruct the input

- feature visualization & image generation (after 0.1 release)

- random forests (after 0.1 release)

- thread_loop with states should use std::future to obtain the returned values (e.g. gradient & loss value)
	

