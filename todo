- layer_t/model_t: idims(), odims(), irows(), icols(), isize(), osize() !!!

- softmax layer: update clasnll loss, may remove the logistic losses

- reduce the number of input/output buffers for consecutive layers: use the output of a layer as the input for the next

- new datasets: INRIA car and person datasets, dymlerchrysler human detection

- more efficient way to store & select samples for large datasets:
	- allocate xGB of RAM to store some samples in memory and load the others from the disk

- function to generate a random input that will closely output a given target (class):
	- can be used to check the learning

- thread_loop with states should use std::future to obtain the returned values (e.g. gradient & loss value)
	
- implement other training methods for the convolution network:
	- unsupervised training:
		- new loss (without the output layer): a single output should have a much larger value magnitude than the others 
			(e.g. learn to disintangle the variation modes)
		- than train the output layer with the annotated samples

	- regularization: variational learning (like in ebbbost)
        - regularization: l2, symmetry

- make install:
	- also copy the headers 


