* investigate why the stochastic trainer is not converging for >=2 hidden layers!!!

 TODO: make the stochastic & minibatch trainers work (VERY hard to tune, poor performance compare to the batch version)

* refactor the trainer_t::v* functions:
	- too much duplicated code!
				
* serializer: store data in std::vector, push_back values -> (serializer_t() << s << v << m).data() or .size()
* same for deserializer
	- BETTER name for serializer! -> vectorizer?!

* test program to print model (ncv_describe -> model structure + convolutions as images)

* thread_loop with states should use std::future to obtain the returned values (e.g. gradient & loss value)
	
idea: variational learning (ebbbost)
idea: regularized (bound) loss with the same approach as in nips12a.pdf

* svhn decoding:
	- the sections are not decoded correctly: 
		- the data type of the last two sections is not valid
		- the number of bytes in the last section is not valid

* could use std::atexit() to save the model at closing -> re-training should reload the current model and continue training
* better could catch various signals: http://en.cppreference.com/w/cpp/utility/program and safely save the trained model

- perf result:   
    66.69%  ncv_trainer  libnanocv.so         [.] ncv::conv_layer_t::backward(ncv::tensor3d_t const&) const                                                                  
    26.76%  ncv_trainer  libnanocv.so         [.] ncv::conv_layer_t::forward(ncv::tensor3d_t const&) const                                                                   
     3.14%  ncv_trainer  libnanocv.so         [.] ncv::fun1_activation_t::value(double) const                                                                                
     1.07%  ncv_trainer  libnanocv.so         [.] ncv::fun1_activation_t::vgrad(double) const

* make install:
	- also copy the headers 

- implement other training methods for the convolution network:
	- auto-encoding: using the unlabeled data (could also try with all samples)
	- train a single layer at a time
	- regularization: l2, symmetry
	- could use genetic algorithms and encode a convolution matrix using e.g. 16 possible values, can impose symmetry-like constrains 
	- could add options to filter the input: e.g. smoothing, normalization, contrast equalization

	- autoencoders: how to generate samples ?!

- add other datasets:
	- INRIA car and person datasets
	- NORB, inria pedestrian detection, inria human detection, dymlerchrysler human detection, face detection, PASCALVoc2011, CALTECH101...

