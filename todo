- easier way and less verbose of creating networks

- (soft) max(abs)-pooling layer: update test programs (check gradient) & training scripts

- filter interface that keep the size of the input image:
	- gaussian, elastic deformations, rotate & translate
	- should be possible to chain them: image(gaussian_t, 2.0)(translate_t, -1, +1)(elastic, 0.0, 1.0)
	- update test program to verify them

- augment the training dataset with random translations and rotations (only for the stochastic trainer) + test program (given image, save N random translations and rotations of a given pixel radius)

- faster ::backward() -> almost 4 times slower than ::forward()
- check if the convolution layers can use fewer buffers (e.g. do not store xdata and update directly the output buffers)
- faster convolution layer - reduce the number of loops - concatenate convolutions?!
       
- thread_loop with states should use std::future to obtain the returned values (e.g. gradient & loss value)
	
- svhn decoding:
	- the sections are not decoded correctly: 
		- the data type of the last two sections is not valid
		- the number of bytes in the last section is not valid

- implement other training methods for the convolution network:
	- regularization: variational learning (like in ebbbost)
        - auto-encoding: using the unlabeled data (could also try with all samples)
        - train a single layer at a time
        - regularization: l2, symmetry
        - could use genetic algorithms and encode a convolution matrix using e.g. 16 possible values, can impose symmetry-like constrains 
        - could add options to filter the input: e.g. smoothing, normalization, contrast equalization

        - autoencoders: how to generate samples ?!

- add other datasets:
        - INRIA car and person datasets
        - NORB, inria pedestrian detection, inria human detection, dymlerchrysler human detection, face detection, PASCALVoc2011, CALTECH101...

- make install:
	- also copy the headers 


