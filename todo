- create batch & stochastic & average stochastic model_t-based derived classes:
	- ::prune_samples(samples_t -> isamples)
	- ::value(samples, isamples) + ::vgrad(samples, isamples), ::load_params(x), ::save_params(x)
	- ::log(result_t)

	- batch: vgrad uses all samples and lbfgs
	- stochastic: vgrad uses a single sample + N(1-8) x n_samples for optimization + x% x n_samples for tuning

- (average) stochastic gradient descent:
	- problem_t::f() -> compute the function for all samples (every N iterations)
	- problem_t::g() -> compute the gradient for a random sample
	- tune the learning rate on the given number of samples

- test program to check the backward & forward step:
	- double check the gradient backward part of the activation functions
		and the math at the layer::backward() part

	- conclusion: 
		- the gradient estimation error increases with both the input range and the number of layers

	- check for better approximations of the gradient!

	- the tanh saturates very fast (needs smaller inputs)!

	- still problems with maxzero!	

- convolution model parameters:
	- number of iterations and epsilon
	- save this to model file (the number of iterations remaining)

- train -> save at closing -> load model -> train from the last parameters & remaining optimization iterations

- MUCH faster optimization (profile the code, it takes more time than without tensors):
	- use noalias for block -> check if any improvement:
		- mat.noalias().block() += ...	

	- bug: the loss becomes infinity sometimes (the line-search step overshoots?!)
	- bug: too many line-search steps => the gradient may be incorrectly computed

	- focus on the backward step: it takes 50% of the processing time
	
	- checkout the convergence speed, experiment with gradient descent and conjugate gd

	- 6s:xxxms for 5000 samples & 8:8x8 hidden layer!
	- 2s:600ms for 2000 samples & 8:8x8 hidden layer!

- option to save the model at closing the program - destructor:
	- trainer().set_path().train()
		- use random initialization if the path is invalid
	- otherwise continue training

- constrain the loss such that the optimization works properly when the number of parameters
	is larger than the number of samples

- convolution network:
	- implement the LBFGS / backprop trainer
	- choose activation from command line

	- make the affine model a specialization of the convolution layer (e.g. no hidden layer)

* image_t::load(matrix_t, color_min = black, color_max = white) -> save the given matrix/filter as a grayscale (by default) image
	- can be used to verify the learned weights/convolutions

- install:
	- also copy the headers 

- update the experimentation script:
	- evaluate some small networks (with 0/1/2... hidden layers), make sure that it works for no hidden layers (like before)
	- evaluate some activation functions

- implement other training methods for the convolution network:
	- >>> derive from the base convolution model and override ::train()
	- auto-encoding: using the unlabeled data (could also try with all samples)
	- train a single layer at a time
	- regularization: l2, symmetry
	- could use genetic algorithms and encode a convolution matrix using e.g. 16 possible values, can impose symmetry-like constrains 
	- could add options to filter the input: e.g. smoothing, normalization, contrast equalization

	- autoencoders: how to generate samples ?!

- small ui application to visualize task images

- faster RGB to CIELab transform (goal: 1s single-thread version of the color testing program)

- add other datasets:
	- INRIA car and person datasets
	- NORB, inria pedestrian detection, inria human detection, dymlerchrysler human detection, face detection, PASCALVoc2011, CALTECH101...

