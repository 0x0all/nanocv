- rename NAG to AG (accelerated gradient):
	- NAG is used in the "Normalized online learning" paper

- update readme (adagrad, adadelta, nag became ag)

- modify the train_mnist.sh script to compare:
	- various optimization methods for each model
	=> can analyze how fast the training loss is decreased using various optimizers
	=> can figure out if using fewer epochs (e.g. 8-16) is enough, if tuning the regularizer

- test & benchmark all stochastic methods (make sure they all work OK - especially NAG & AdaGrad)

- implement AdaDelta:
	- move the epsilon definition (sqrt of limits) to base_t::epsilon() to be reused by AdaGrad

- may modify the running-weighted-average to use the exponential scheme from AdaDelta:
	- e.g. ro = 0.95 by default

	- or can try a more numerically robust weighting scheme: f(k / (epochs * epoch_size))

- other stochastic methods to implement:
	- http://arxiv.org/pdf/1206.1106v2.pdf
	- http://www.matthewzeiler.com/pubs/googleTR2012/googleTR2012.pdf

- improve the dummy task to generate meaningful input images:
	- this way the optimization test program will actually work on meaninful data

- check the speed improvement if using floats instead of doubles for training MLP0

- investigate why AdaGrad presents that strange bump in the training loss plot

- investigate why NAG is so noisy

- augment training samples (to reduce overfitting):
	- salt & pepper noise (vary the percentage)
	- translation, scale, various deformations

	- test program to generate some augmented training samples

- more experiments with floats instead of doubles:
	- check the impact on MLPs and convolutions
	- are the stochastic methods going to be faster?! (tons of gradient-based linear algebra)

- MAD with index shifts (may speed-up corr2d)

- test program to assess the speed of the 3D convolution/correlation operations

- faster convolution:
 	- build a toeplitz matrix, then the convolution becomes a matrix multiplication:
		- http://en.wikipedia.org/wiki/Toeplitz_matrix

        - check if any improvement if using a 3D convolution    

- GTSD dataset support (after 0.1 release):
	- create both classification and detection tasks
	- build an object detector

- finish the training scripts for other datasets (CIFAR-10, CIFAR-100, SVHN, NORB, STL)

- unsupervised learning (after 0.1 release): 
	- unsupervised training:
		- new loss (without the output layer): a single output should have a much larger value magnitude than the others 
			(e.g. learn to disintangle the variation modes)
		- than train the output layer with the annotated samples

- reconstruction regularization (after 0.1 release):
	- for linear & convolution layers -> reuse parameters to easily reconstruct the input

- feature visualization & image generation (after 0.1 release)

- thread_loop with states should use std::future to obtain the returned values (e.g. gradient & loss value)
	

